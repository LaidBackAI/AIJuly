# 06-2 | k-평균

```python
# KMeans 클래스
!wget https://bit.ly/fruits_300 -O fruits_300.npy

import numpy as np
fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100)

from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)

import matplotlib.pyplot as plt
def draw_fruits(arr, ratio=1):
  n = len(arr) # n은 샘플 개수
  # 한 줄에 10개씩 이미지를 그린다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산한다
  rows = int(np.ceil(n/10))
  # 행이 1개이면 열의 개수는 샘플 개수이다. 그렇지 않으면 10개
  cols = n if rows < 2 else 10
  fig, axs = plt.subplots(rows, cols,
                          figsize=(cols*ratio, rows*ratio), squeeze=False)
  for i in range(rows):
    for j in range(cols):
      if i*10 + j < n: # n개까지만 그린다
        axs[i, j].imshow(arr[i*10 + j], cmap = 'gray_r')
      axs[i, j].axis('off')
  plt.show()

draw_fruits(fruits[km.labels_==0])
```

![06-2%20k-%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%B2%E1%86%AB%20408932cde2324f0aa12be680be7d8ff4/Untitled.png](06-2%20k-%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%B2%E1%86%AB%20408932cde2324f0aa12be680be7d8ff4/Untitled.png)

```python
draw_fruits(fruits[km.labels_==1])
```

![06-2%20k-%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%B2%E1%86%AB%20408932cde2324f0aa12be680be7d8ff4/Untitled%201.png](06-2%20k-%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%B2%E1%86%AB%20408932cde2324f0aa12be680be7d8ff4/Untitled%201.png)

```python
draw_fruits(fruits[km.labels_==2])
```

![06-2%20k-%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%B2%E1%86%AB%20408932cde2324f0aa12be680be7d8ff4/Untitled%202.png](06-2%20k-%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%B2%E1%86%AB%20408932cde2324f0aa12be680be7d8ff4/Untitled%202.png)

```python
# 클러스터 중심
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)
```

![06-2%20k-%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%B2%E1%86%AB%20408932cde2324f0aa12be680be7d8ff4/Untitled%203.png](06-2%20k-%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%B2%E1%86%AB%20408932cde2324f0aa12be680be7d8ff4/Untitled%203.png)

```python
# 최적의 k 찾기
inertia = []
for k in range(2, 7):
  km = KMeans(n_clusters = k, random_state=42)
  km.fit(fruits_2d)
  inertia.append(km.inertia_)
plt.plot(range(2, 7), inertia)
plt.show과
```

![06-2%20k-%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%B2%E1%86%AB%20408932cde2324f0aa12be680be7d8ff4/Untitled%204.png](06-2%20k-%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%B2%E1%86%AB%20408932cde2324f0aa12be680be7d8ff4/Untitled%204.png)

이전에 과일 종류별로 픽셀 평균값을 계산했다. 하지만 실전에서는 어떤 과일 사진이 들어올지 모른다. 따라서 타깃값을 모르는 척하고 자동으로 사진을 클러스터로 모을 수 있는 군집 알고리즘이 필요하다.

대표적인 군집 알고리즘인 k-평균 알고리즘을 사용했다. k-평균은 비교적 간단하고 속도가 빠르며 이해하기도 쉽다. k-평균 알고리즘을 구현한 사이킷런의 KMeans 클래스는 각 샘플이 어떤 클러스터에 소속되어 있는지 labels_ 속성에 저장한다.

각 샘플에서 각 클러스터까지의 거리를 하나의 특성으로 활용할 수도 있다. 이를 위해 KMeans 클래스는 transform() 메서드를 제공한다. 또한 predict() 메서드에서 새로운 샘플에 대해 가장 가까운 클러스터를 예측값으로 출력한다.

k-평균 알고리즘은 사전에 클러스터 개수를 미리 지정해야 한다. 사실 데이터를 직접 확인하지 않고서는 몇 개의 클러스터가 만들어질지 알기 어렵다. 최적의 클러스터 개수 k를 알아내는 한 가지 방법은 클러스터가 얼마나 밀집되어 있는지 나타내는 이너셔를 사용하는 것이다. 이너셔가 더이상 크게 줄어들지 않는다면 클러스터 개수를 더 늘리는 것은 효과가 없다. 이를 엘보우 방법이라고 부른다.

사이킷런의 KMeans 클래스는 자동으로 이너셔를 계산하여 inertia_ 속성으로 제공한다. 클러스터 개수를 늘리면서 반복하여 KMeans 알고리즘을 훈련하고 이너셔가 줄어드는 속도가 꺾이는 지점을 최적의 클러스터 개수로 결정한다.

k-평균 알고리즘의 클러스터 중심까지 거리를 특성으로 사용할 수도 있다는 점을 보았다. 이렇게 하면 훈련 데이터의 차원을 크게 줄일 수 있다. 데이터셋의 차원을 줄이면 지도 학습 알고리즘의 속도를 크게 높일 수 있다.

- **k-평균** 알고리즘은 처음에 랜덤하게 클러스터 중심을 정하고 클러스터를 만든다. 그다음 클러스터의 중심을 이동하고 다시 클러스터를 만드는 식으로 반복해서 최적의 클러스터를 구성하는 알고리즘이다.
- **클러스터 중심**은 k-평균 알고리즘이 만든 클러스터에 속한 샘플의 특성 평균값이다. centroid라고도 부른다. 가장 가까운 클러스터 중심을 샘플의 또 다른 특성으로 사용하거나 새로운 샘플에 대한 예측으로 활용할 수 있다.
- **엘보우 방법**은 최적의 클러스터 개수를 정하는 방법 중 하나이다. 이너셔는 클러스터 중심과 샘플 사이 거리의 제곱 합이다. 클러스터 개수에 따라 이너셔 감소가 꺾이는 지점이 적절한 클러스터 개수 k가 될 수 있다. 이 그래프의 모양을 따서 엘보우 방법이라고 부른다.