# 05-1 | 결정 트리

```python
import pandas as pd
wine = pd.read_csv('https://bit.ly/wine-date')

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```

![05-1%20%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%B5%20ec80cd3ac488403196819774dd361ae9/Untitled.png](05-1%20%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%B5%20ec80cd3ac488403196819774dd361ae9/Untitled.png)

```python
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 
                                                       'sugar', 'pH'])
plt.show()
```

![05-1%20%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%B5%20ec80cd3ac488403196819774dd361ae9/Untitled%201.png](05-1%20%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%B5%20ec80cd3ac488403196819774dd361ae9/Untitled%201.png)

```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

![05-1%20%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%B5%20ec80cd3ac488403196819774dd361ae9/Untitled%202.png](05-1%20%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%B5%20ec80cd3ac488403196819774dd361ae9/Untitled%202.png)

```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

![05-1%20%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%B5%20ec80cd3ac488403196819774dd361ae9/Untitled%203.png](05-1%20%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%90%E1%85%B3%E1%84%85%E1%85%B5%20ec80cd3ac488403196819774dd361ae9/Untitled%203.png)

머신러닝 모델을 종종 블랙박스와 같다고 말한다. 실제로 모델의 계수나 절편이 왜 그렇게 학습되었는지 설명하기가 어렵다. 이에 비해 결정 트리는 비교적 비전문가에게도 설명하기 쉬운 모델을 만든다. 하지만 결정 트리는 여기에서 끝이 아니다. 결정 트리는 많은 앙상블 학습 알고리즘의 기반이 된다. 앙상블 학습은 신경망과 함께 가장 높은 성능을 내기 때문에 인기가 높은 알고리즘이다.

- **결정 트리**는 예/아니오에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘이다. 비교적 예측 과정을 이해하기 쉽고 성능도 뛰어나다.
- **불순도**는 결정 트리가 최적의 질문을 찾기 위한 기준이다. 사이킷런은 지니 불순도와 엔트로피 불순도를 제공한다.
- **정보 이득**은 부모 노드와 자식 노드의 불순도 차이이다. 결정 트리 알고리즘은 정보 이득이 최대화되도록 학습한다.
- 결정 트리는 제한 없이 성장하면 훈련 세트에 과대적합되기 쉽다. **가지치기**는 결정 트리의 성장을 제한하는 방법이다. 사이킷런의 결정 트리 알고리즘은 여러 가지 가지치기 매개변수를 제공한다.
- **특성 중요도**는 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값이다. 특성 중요도를 계산할 수 있는 것이 결정 트리의 또다른 큰 장점이다.