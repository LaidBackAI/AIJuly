# 07-2 | 심층 신경망

```python
# 2개의 층
from tensorflow import keras
(train_input, train_target), (test_input, test_target) =\
    keras.datasets.fashion_mnist.load_data()

from sklearn.model_selection import train_test_split
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)
train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))
dense2 = keras.layers.Dense(10, activation='softmax')

# 심층 신경망 만들기
model = keras.Sequential([dense1, dense2])

model.summary()

# 층을 추가하는 다른 방법
model = keras.Sequential([
                          keras.layers.Dense(100, activation='sigmoid', input_shape=(784,),
                                             name='hidden'),
                          keras.layers.Dense(10, activation='softmax', name='output')
], name='패션 MNIST 모델')

model.summary()

model = keras.Sequential()
model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()

model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
model.fit(train_scaled, train_target, epochs=5)

# 렐루 함수
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()

(train_input, train_target), (test_input, test_target) =\
    keras.datasets.fashion_mnist.load_data()
train_scaled = train_input / 255.0
train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
model.fit(train_scaled, train_target, epochs=5)

model.evaluate(val_scaled, val_target)

# 옵티마이저
model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', 
              metrics='accuracy')

sgd = keras.optimizers.SGD()
model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy',
              metrics = 'accuracy')

sgd = keras.optimizers.SGD(learning_rate=0.1)

sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)

adagrad = keras.optimizers.Adagrad()
model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy',
              metrics='accuracy')

rmsprop = keras.optimizers.RMSprop()
model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy',
              metrics='accuracy')

model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28,28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics='accuracy')
model.fit(train_scaled, train_target, epochs=5)

model.evaluate(val_scaled, val_target)
```

여러 개의 층을 추가하여 다층 인공 신경망을 만드는 방법을 터득했다. 특별히 이런 인공 신경망을 심층 신경망이라고 부른다. 또 케라스 API를 사용하여 층을 추가하는 여러 가지 방법을 알아보았다.

케라스 모델의 정보를 요약해 주는 summary() 메서드를 사용해 보았다. 출력값의 의미를 이해하고 모델 파라미터 개수를 계산해 맞추어 보았다. 모델 파라미터 개수를 계산하는 과정은 모델을 올바르게 이해하고 있는지 확인하는 좋은 방법 중 하나이다.

은닉층에 적용한 시그모이드 활성화 함수 대신에 새로운 렐루 활성화 함수에 대해 배웠고 이를 적용해 약간의 성능을 향상시켰다. 또한 다양한 고급 경사 하강법 옵티마이저들을 적용하는 방법도 살펴보았다. 케라스 API를 사용하면 이런 작업이 어렵지 않고 직관적으로 구성할 수 있다.

- **심층 신경망**은 2개 이상의 층을 포함한 신경망이다. 종종 다층 인공 신경망, 심층 신경망, 딥러닝을 같은 의미로 사용한다.
- **렐루 함수**는 이미지 분류 모델의 은닉층에 많이 사용하는 활성화 함수이다. 시그모이드 함수는 층이 많을수록 활성화 함수의 양쪽 끝에서 변화가 작기 때문에 학습이 어려워진다. 렐루 함수는 이런 문제가 없으며 계산도 간단하다.
- **옵티마이저**는 신경망의 가중치와 절편을 학습하기 위한 알고리즘 또는 방법을 말한다. 케라스에는 다양한 경사 하강법 알고리즘이 구현되어 있다. 대표적으로 SGD, 네스테로프 모멘텀, RMSprop, Adam 등이 있다.