# 06-3 | 주성분 분석

```python
# PCA 클래스
!wget https://bit.ly/fruits_300 -O fruits_300.npy
import numpy as np
fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100)

from sklearn.decomposition import PCA
pca = PCA(n_components = 50)
pca.fit(fruits_2d)

import matplotlib.pyplot as plt
def draw_fruits(arr, ratio=1):
  n = len(arr) # n은 샘플 개수
  # 한 줄에 10개씩 이미지를 그린다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산한다
  rows = int(np.ceil(n/10))
  # 행이 1개이면 열의 개수는 샘플 개수이다. 그렇지 않으면 10개
  cols = n if rows < 2 else 10
  fig, axs = plt.subplots(rows, cols,
                          figsize=(cols*ratio, rows*ratio), squeeze=False)
  for i in range(rows):
    for j in range(cols):
      if i*10 + j < n: # n개까지만 그린다
        axs[i, j].imshow(arr[i*10 + j], cmap = 'gray_r')
      axs[i, j].axis('off')
  plt.show()

draw_fruits(pca.components_.reshape(-1, 100, 100))
```

![Untitled](https://user-images.githubusercontent.com/87055471/128161525-a2389f75-290d-4e04-bc0f-bc302f9c90e0.png)

```python
fruits_pca = pca.transform(fruits_2d)

# 원본 데이터 재구성
fruits_inverse = pca.inverse_transform(fruits_pca)

fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)
for start in [0, 100, 200]:
  draw_fruits(fruits_reconstruct[start:start+100])
  print("\n")
```
![Untitled 1](https://user-images.githubusercontent.com/87055471/128161539-6e5cc6b8-ba91-400a-8b3b-3703f74ea355.png)
![Untitled 2](https://user-images.githubusercontent.com/87055471/128161558-fccac5ba-8d1a-439e-a6f0-0d33c7e9d03d.png)

![Untitled 3](https://user-images.githubusercontent.com/87055471/128161568-0b1fae02-ddf8-4af2-bee6-c3733b6b79ac.png)

```python
# 설명된 분산
plt.plot(pca.explained_variance_ratio_)
```

![Untitled 4](https://user-images.githubusercontent.com/87055471/128161577-51c8d5c9-fbe5-4a75-88c2-b86603556316.png)

```python
# 다른 알고리즘과 함께 사용하기
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

target = np.array([0]*100 + [1]*100 + [2]*100)

from sklearn.model_selection import cross_validate
scores = cross_validate(lr, fruits_2d, target)

scores = cross_validate(lr, fruits_pca, target)

pca = PCA(n_components = 0.5)
pca.fit(fruits_2d)

fruits_pca = pca.transform(fruits_2d)

scores = cross_validate(lr, fruits_pca, target)

from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_pca)

for label in range(0, 3):
  draw_fruits(fruits[km.labels_ == label])
  print("\n")
```

![Untitled 5](https://user-images.githubusercontent.com/87055471/128161595-bfb562e7-1658-4471-85b6-644274717d5d.png)

![Untitled 6](https://user-images.githubusercontent.com/87055471/128161605-bb678ea5-c4bb-46d5-9d06-2ca8848bc5e5.png)

![Untitled 7](https://user-images.githubusercontent.com/87055471/128161613-303cd658-d160-4bd5-8d84-9abcb9841c7c.png)

```python
for label in range(0, 3):
  data = fruits_pca[km.labels_ == label]
  plt.scatter(data[:,0], data[:,1])
plt.legend(['apple', 'banana', 'pineapple'])
plt.show()
```

![Untitled 8](https://user-images.githubusercontent.com/87055471/128161629-ad7eeb60-85e9-462c-9e8c-9d323c452ac2.png)

대표적인 비지도 학습 문제 중 하나인 차원 축소에 대해 알아보았다. 차원 축소를 사용하면 데이터셋의 크기를 줄일 수 있고 비교적 시각화하기 쉽다. 또 차원 축소된 데이터를 지도 학습 알고리즘이나 다른 비지도 학습 알고리즘에 재사용하여 성능을 높이거나 훈련 속도를 빠르게 만들 수 있다.

사이킷런의 PCA 클래스를 사용해 과일 사진 데이터의 특성을 50개로 크게 줄였다. 특성 개수는 작지만 변환된 데이터는 원본 데이터에 있는 분산의 90% 이상을 표현한다. 이를 설명된 분산이라 부른다.

PCA 클래스는 자동으로 설명된 분산을 계산하여 제공해 준다. 또한, 주성분의 개수를 명시적으로 지정하는 대신 설명된 분산의 비율을 설정하여 원하는 비율만큼 주성분을 찾을 수 있다.

PCA 클래스는 변환된 데이터에서 원본 데이터를 복원하는 메서드도 제공한다. 변환된 데이터가 원본 데이터의 분산을 모두 유지하고 있지 않다면 완벽하게 복원되지 않는다. 하지만 적은 특성으로도 상당 부분의 디테일을 복원할 수 있다.

- **차원 축소**는 원본 데이터의 특성을 적은 수의 새로운 특성으로 변환하는 비지도 학습의 한 종류이다. 차원 축소는 저장 공간을 줄이고 시각화하기 쉽다. 또한, 다른 알고리즘의 성능을 높일 수도 있다.
- **주성분 분석**은 차원 축소 알고리즘의 하나로 데이터에서 가장 분산이 큰 방향을 찾는 방법이다. 이런 방향을 주성분이라고 부른다. 원본 데이터를 주성분에 투영하여 새로운 특성을 만들 수 있다. 일반적으로 주성분은 원본 데이터에 있는 특성 개수보다 작다.
- **설명된 분산**은 주성분 분석에서 주성분이 얼마나 원본 데이터의 분산을 잘 나타내는지 기록한 것이다. 사이킷런의 PCA 클래스는 주성분 개수나 설명된 분산의 비율을 지정하여 주성분 분석을 수행할 수 있다.
